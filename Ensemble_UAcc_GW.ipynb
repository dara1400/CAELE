{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"AksfUhpdVjQj"},"outputs":[],"source":["import os\n","import sys\n","import copy\n","import time\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","import torch\n","import torch.multiprocessing as mp\n","from torch.autograd import Variable\n","from torch.distributions import constraints\n","from torch.utils.data import DataLoader, random_split, Dataset, TensorDataset\n","\n","from sklearn.metrics import accuracy_score\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","#******************************************************************************\n","\n","main_folder = '/content/drive/MyDrive/Ensemble_UAcc_GW'\n","sys.path.append(f'{main_folder}/')\n","from utils.metrics import *\n","from utils.optimizers import *\n","from utils.uncertainty import *\n","from utils.generate_datasets import *\n","datasets_folder = f'{main_folder}/datasets'\n","results_folder = f'{main_folder}/results'\n","models_folder = f'{main_folder}/models'\n","dataset_names = [f.replace('.pkl', '') for f in os.listdir(datasets_folder) if 'moons' in f or 'blobs' in f]\n","dataset_names.append('myocarditis')\n","dataset_names.append('alzheimer_2classes_mri')\n","target_device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","\n","#******************************************************************************\n","\n","optimized_parameters_file = f'{results_folder}/optimized_parameters.csv'\n","if os.path.isfile(optimized_parameters_file):\n","    optimized_parameters = pd.read_csv(optimized_parameters_file)\n","else:\n","    optimized_parameters = pd.DataFrame(columns=['dataset_name', 'optimizer', 'c0', 'c1', 'c2', 'c3', 'c4'])\n","\n","preds_file = f'{results_folder}/preds.pkl'\n","metrics_file = f'{results_folder}/metrics.csv'\n","\n","#******************************************************************************\n","\n","def load_synthetic_dataset(datasets_folder, dataset_name):\n","    dataset_path = f'{datasets_folder}/{dataset_name}.pkl'\n","    with open(dataset_path, 'rb') as f:\n","        [x_train, y_train, x_test, y_test] = pickle.load(f)\n","\n","    # print(x_train.shape, y_train.shape, np.count_nonzero(y_train))\n","    # print(x_test.shape, y_test.shape, np.count_nonzero(y_test))\n","\n","    x_train_ready = torch.Tensor(x_train).float()\n","    y_train_ready = torch.Tensor(y_train).long()\n","    x_test_ready  = torch.Tensor(x_test).float()\n","    y_test_ready  = torch.Tensor(y_test).long()\n","\n","    train_loader = DataLoader(dataset=TensorDataset(x_train_ready, y_train_ready),  batch_size=1024, shuffle=True)\n","    test_loader  = DataLoader(dataset=TensorDataset(x_test_ready, y_test_ready),  batch_size=1024, shuffle=True)\n","\n","    return train_loader, test_loader\n","\n","#******************************************************************************\n","\n","class MCDropoutModel(torch.nn.Module):\n","    def __init__(self, input_size, output_size, p1, p2, l1, l2):\n","        super(MCDropoutModel, self).__init__()\n","        self.f = torch.nn.Sequential(\n","            torch.nn.Linear(input_size, l1),\n","            torch.nn.Dropout(p=p1),\n","            torch.nn.ReLU(),\n","            torch.nn.Linear(l1, l2),\n","            torch.nn.Dropout(p=p2),\n","            torch.nn.ReLU(),\n","            torch.nn.Linear(l2, output_size))\n","    def forward(self, x):\n","        return self.f(x)\n","\n","#******************************************************************************\n","\n","def train_model(dataset_name, train_loader, test_loader, print_dict, params={'p1':0.25, 'p2':0.25, 'l1':64, 'l2':16}, show_progress=False, pe_entropy_mean_loss=True):\n","    # print('*', end='')\n","    if isinstance(params, dict):\n","        p1, p2, l1, l2 = params['p1'], params['p2'], params['l1'], params['l2']\n","    else:\n","        p1, p2, l1, l2 = params\n","    l1, l2 = int(l1), int(l2)\n","    torch.cuda.empty_cache()\n","    x, y = next(iter(train_loader))\n","    input_size = x.shape[1]\n","    output_size = y.unique().size()[0]\n","    model = MCDropoutModel(input_size, output_size, p1, p2, l1, l2)\n","    model.to(target_device)\n","    optimizer = torch.optim.Adam(model.parameters() ,lr=0.001, eps=1e-07)\n","    criterion = torch.nn.CrossEntropyLoss()\n","\n","    best_acc = 0\n","    best_loss = 10000\n","    save_counter = 0\n","    best_model_parameters = {}\n","    epochs = 200\n","\n","    for epoch in range(epochs):\n","\n","        loss_train = 0\n","        model.train()\n","        for x_batch, y_batch in train_loader:\n","            x_batch = x_batch.to(target_device)\n","            y_batch = y_batch.to(target_device)\n","            optimizer.zero_grad()\n","            y_pred_out = model(x_batch)\n","            loss = criterion(y_pred_out, y_batch)\n","            if pe_entropy_mean_loss:\n","                y_pred, probs, pe_entropy, final_probs = calc_MCD_entropy(model, x_batch, run_counts=1000)\n","                loss += pe_entropy.mean()\n","            loss.backward()\n","            optimizer.step()\n","            loss_train += loss\n","\n","        correct = 0\n","        loss_test = 0\n","        with torch.no_grad():\n","            model.eval()\n","            for x_batch, y_batch in test_loader:\n","                x_batch = x_batch.to(target_device)\n","                y_batch = y_batch.to(target_device)\n","                y_pred_out = model(x_batch)\n","                loss = criterion(y_pred_out, y_batch)\n","                if pe_entropy_mean_loss:\n","                    y_pred, probs, pe_entropy, final_probs = calc_MCD_entropy(model, x_batch, run_counts=1000)\n","                    loss += pe_entropy.mean()\n","                loss_test += loss\n","                y_pred_softmax = torch.softmax(y_pred_out, dim=1)\n","                y_pred = torch.argmax(y_pred_softmax, dim=1)\n","                correct += (y_pred == y_batch).sum().item()\n","        acc = 100 * correct / len(test_loader.dataset)\n","\n","        if loss_test < best_loss:\n","            best_acc = acc\n","            best_loss = loss_test\n","            save_counter += 1\n","            best_model_parameters = copy.deepcopy(model.state_dict())\n","\n","        print_dict[dataset_name] = 'EP:%0d,TL:%0.5f,Acc:%0.2f' % (epoch, loss, best_acc)\n","        if epoch % 1 == 0 and show_progress:\n","            print('\\rDataset Name: %s, Epoch: %0d, Test loss: %0.5f, Best Accuracy: %0.2f, Save Counter: %0d' % (dataset_name, epoch, loss, best_acc, save_counter), end='')\n","    del print_dict[dataset_name]\n","\n","    return best_model_parameters, best_loss\n","\n","#******************************************************************************\n","\n","l1_l2_list = [(87, 22), (78, 22), (98, 30), (117, 28), (111, 18)]\n","p1, p2 = 0.25, 0.25\n","\n","def train_dataset_models(dataset_name, print_dict, show_progress):\n","    train_loader, test_loader = load_synthetic_dataset(datasets_folder, dataset_name)\n","    for model_number in range(len(l1_l2_list)):\n","        model_file = f'{models_folder}/mcd_model_{dataset_name}_{model_number}.pth'\n","        if os.path.isfile(model_file):\n","            continue\n","        # print('\\n', model_number, dataset_name)\n","        l1, l2 = l1_l2_list[model_number]\n","        params = [p1, p2 , l1, l2]\n","        best_model_parameters, best_loss = train_model(f'{dataset_name}_{model_number}', train_loader, test_loader\n","                                                       , print_dict, params, show_progress)\n","        torch.save(best_model_parameters, model_file)\n","\n","#******************************************************************************\n","\n","def load_model(model_file, p1, p2 , l1, l2, test_loader, target_device):\n","    x, y = next(iter(test_loader))\n","    input_size = x.shape[1]\n","    output_size = y.unique().size()[0]\n","    model = MCDropoutModel(input_size, output_size, p1, p2 , l1, l2)\n","    model.to(target_device)\n","    model.load_state_dict(torch.load(model_file, map_location=target_device))\n","    return model\n","\n","#******************************************************************************\n","\n","def calc_ensemble_entropy(dataset_name, model_COs, test_loader):\n","\n","    models = []\n","    for model_number in range(len(l1_l2_list)):\n","        model_file = f'{models_folder}/mcd_model_{dataset_name}_{model_number}.pth'\n","        l1, l2 = l1_l2_list[model_number]\n","        model = load_model(model_file, p1, p2 , l1, l2, test_loader, target_device)\n","        model.eval()\n","        models.append(model)\n","\n","    final_probs = []\n","    y_test = []\n","    with torch.no_grad():\n","        for x_batch, y_batch in test_loader:\n","            x_batch = x_batch.to(target_device)\n","            y_test.append(y_batch)\n","            probs_batch = 0\n","            for model_number, model in enumerate(models):\n","                y_pred_out = model(x_batch)\n","                probs_batch += model_COs[model_number] * torch.softmax(y_pred_out, dim=1)\n","            final_probs.append(probs_batch)\n","\n","    y_test = torch.hstack(y_test)\n","    final_probs = torch.vstack(final_probs)\n","    y_pred = torch.argmax(final_probs, dim=1)\n","    pe_entropy  = calc_entropy(final_probs)\n","\n","    y_test = y_test.numpy().squeeze()\n","    y_pred = y_pred.cpu().detach().numpy()\n","    pe_entropy = pe_entropy.cpu().detach().numpy()\n","    final_probs = final_probs.cpu().detach().numpy()\n","\n","    return y_test, y_pred, pe_entropy, final_probs\n","\n","#******************************************************************************\n","\n","def calc_ensemble_UAcc(dataset_name, model_COs):\n","    train_loader, test_loader = load_synthetic_dataset(datasets_folder, dataset_name)\n","    y_test, y_pred, pe_entropy, final_probs = calc_ensemble_entropy(dataset_name, model_COs, test_loader)\n","    TU, FC, FU, TC, UAcc, USen, USpe, UPre = calc_confusion_uncertainty_matrix(y_pred, y_test, pe_entropy, pe_entropy_thresh=0.3)\n","    return UAcc\n","\n","#******************************************************************************\n","\n","def calc_dataset_MCD_entropy(model, dataset_name, run_counts):\n","    train_loader, test_loader = load_synthetic_dataset(datasets_folder, dataset_name)\n","    l1, l2 = l1_l2_list[model_number]\n","    model = load_model(model_file, p1, p2 , l1, l2, test_loader, target_device)\n","    model.train()\n","\n","    final_probs = []\n","    y_test = []\n","    with torch.no_grad():\n","        for x_batch, y_batch in test_loader:\n","            x_batch = x_batch.to(target_device)\n","            y_test.append(y_batch)\n","            _, _, _, probs_batch = calc_MCD_entropy(model, x_batch, run_counts=1000)\n","            final_probs.append(probs_batch)\n","\n","    y_test = torch.hstack(y_test)\n","    final_probs = torch.vstack(final_probs)\n","    y_pred = torch.argmax(final_probs, dim=1)\n","    pe_entropy  = calc_entropy(final_probs)\n","\n","    y_test = y_test.numpy().squeeze()\n","    y_pred = y_pred.cpu().detach().numpy()\n","    pe_entropy = pe_entropy.cpu().detach().numpy()\n","    final_probs = final_probs.cpu().detach().numpy()\n","\n","    return y_test, y_pred, pe_entropy, final_probs\n","\n","\n","#******************************************************************************\n","\n","def calc_save_metrics(metrics, dataset_name, model_name, y_test, y_pred, pe_entropy, final_probs):\n","    _, _, ECE = calc_ECE(y_test, y_pred, final_probs)\n","    Acc = accuracy_score(y_test, y_pred)\n","    TU, FC, FU, TC, UAcc, USen, USpe, UPre = calc_confusion_uncertainty_matrix(y_pred, y_test, pe_entropy, pe_entropy_thresh=0.5)\n","    r = {'dataset_name': dataset_name, 'model_name': model_name, 'Acc':f'{Acc*100:0.2f}', 'UAcc': f'{UAcc*100:0.2f}', 'ECE': f'{ECE:0.2f}'}\n","    r = pd.DataFrame(r, index=[0])\n","    metrics = pd.concat([metrics, r], ignore_index=True)\n","    return metrics\n","\n","#******************************************************************************\n","\n","def plot_kde(dataset_name, model_name, p):\n","    kdefig, axes = plt.subplots(nrows=2, ncols=2, figsize=(10, 10), gridspec_kw={'height_ratios': [1, 3], 'width_ratios': [3, 1]})\n","    plt.subplots_adjust(hspace=0.01)\n","    plt.subplots_adjust(wspace=0.01)\n","    kdefig.delaxes(axes[0,1])\n","\n","    y_test, y_pred, pe_entropy, final_probs = p.y_test, p.y_pred, p.pe_entropy, p.final_probs\n","    corr_class_indexes =  np.where(y_pred == y_test)\n","    miss_class_indexes =  np.where(y_pred != y_test)\n","\n","    sns.distplot(pe_entropy[corr_class_indexes], hist = False, kde = True, kde_kws = {'linewidth': 1.5}, color='b' ,label='', ax=axes[0,0])\n","    sns.distplot(pe_entropy[miss_class_indexes], hist = False, kde = True, kde_kws = {'linewidth': 1.5}, color='r', label='', ax=axes[0,0])\n","    axes[0,0].set_xticks([])\n","    axes[0,0].set_yticks([])\n","    axes[0,0].set_xlabel('')\n","    axes[0,0].set_ylabel('')\n","\n","    sns.distplot(final_probs.max(axis=1)[corr_class_indexes], hist = False, kde = True, kde_kws = {'linewidth': 1.5}, color='b' ,label='', vertical=True, ax=axes[1,1])\n","    sns.distplot(final_probs.max(axis=1)[miss_class_indexes], hist = False, kde = True, kde_kws = {'linewidth': 1.5}, color='r', label='', vertical=True, ax=axes[1,1])\n","    axes[1,1].set_xticks([])\n","    axes[1,1].set_yticks([])\n","    axes[1,1].set_xlabel('')\n","    axes[1,1].set_ylabel('')\n","\n","    data = pd.DataFrame({'Predictive Entropy': pe_entropy, 'Predictive Probability': final_probs.max(axis=1), 'prediction': y_pred==y_test})\n","    data['prediction'] = data['prediction'].replace(True, 'CorrectClass').replace(False, 'MisClass')\n","    sns.kdeplot(data=data, x='Predictive Entropy', y='Predictive Probability', hue='prediction', levels=4, thresh=.1, common_norm=False, ax=axes[1,0]).legend_.set_title(None)\n","    kdefig.savefig(f'{results_folder}/plots/kde_{dataset_name}_{model_name}.pdf', bbox_inches='tight')\n","\n","#******************************************************************************\n","\n","for dataset_name in dataset_names:\n","    train_dataset_models(dataset_name, {}, True)\n","\n","#******************************************************************************\n","\n","optimizer = 'GWO'\n","for dataset_name in dataset_names:\n","    if ((optimized_parameters['dataset_name'] == dataset_name) & (optimized_parameters['optimizer'] == optimizer)).any():\n","        continue\n","    print('\\n', dataset_name, optimizer)\n","    alfa_wolf = gray_wolf_optimizer(dataset_name=dataset_name, fitness_function=calc_ensemble_UAcc, max_iter=100, number_of_wolfs=20, fit_up=True)\n","    c0, c1, c2, c3, c4 = alfa_wolf.params\n","    r = {'dataset_name': dataset_name, 'optimizer': optimizer, 'c0': c0, 'c1':c1, 'c2':c2, 'c3':c3, 'c4':c4}\n","    r = pd.DataFrame(r, index=[0])\n","    optimized_parameters = pd.concat([optimized_parameters, r], ignore_index=True)\n","    optimized_parameters.to_csv(optimized_parameters_file, index=False)\n","\n","# #******************************************************************************"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QC3HD4lh56GV"},"outputs":[],"source":["dataset_names = ['blobs_0.8clusterstd', 'blobs_0.9clusterstd', 'blobs_0.85clusterstd', 'blobs_2clusterstd', 'blobs_2.25clusterstd',\n","                '2moons_0.185noise', '2moons_0.2noise', '2moons_0.25noise', '2moons_0.275noise',\n","                'alzheimer_2classes_mri']\n","\n","if not os.path.isfile(preds_file):\n","\n","    preds = []\n","    optimizer = 'GWO'\n","\n","    for dataset_name in dataset_names:\n","        print(dataset_name, end=',')\n","        optimized_parameter = optimized_parameters[(optimized_parameters['dataset_name'] == dataset_name) & (optimized_parameters['optimizer'] == optimizer)].iloc[0]\n","        c0, c1, c2, c3, c4 = optimized_parameter.c0,  optimized_parameter.c1, optimized_parameter.c2,  optimized_parameter.c3, optimized_parameter.c4\n","        model_COs = c0, c1, c2, c3, c4\n","\n","        for model_number in range(len(l1_l2_list)):\n","            model_file = f'{models_folder}/mcd_model_{dataset_name}_{model_number}.pth'\n","            y_test, y_pred, pe_entropy, final_probs = calc_dataset_MCD_entropy(model_file, dataset_name, run_counts=1000)\n","            preds.append({'dataset_name': dataset_name, 'model_name': model_number, 'y_test':y_test, 'y_pred': y_pred, 'pe_entropy': pe_entropy, 'final_probs':final_probs})\n","\n","        train_loader, test_loader = load_synthetic_dataset(datasets_folder, dataset_name)\n","        y_test, y_pred, pe_entropy, final_probs = calc_ensemble_entropy(dataset_name, [0.2,0.2,0.2,0.2,0.2], test_loader)\n","        preds.append({'dataset_name': dataset_name, 'model_name': 'En', 'y_test':y_test, 'y_pred': y_pred, 'pe_entropy': pe_entropy, 'final_probs':final_probs})\n","\n","        train_loader, test_loader = load_synthetic_dataset(datasets_folder, dataset_name)\n","        y_test, y_pred, pe_entropy, final_probs = calc_ensemble_entropy(dataset_name, model_COs, test_loader)\n","        preds.append({'dataset_name': dataset_name, 'model_name': 'En_GWO', 'y_test':y_test, 'y_pred': y_pred, 'pe_entropy': pe_entropy, 'final_probs':final_probs})\n","\n","    preds = pd.DataFrame(preds)\n","    preds.to_pickle(preds_file)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pjOPUnwYGEO-"},"outputs":[],"source":["preds = pd.read_pickle(preds_file)\n","\n","metrics = pd.DataFrame()\n","plot_names = ['0', '1', '2', '3', '4', 'En', 'En_GWO']\n","# pe_entropy_threshs = np.arange(0.1, 1, 0.1)\n","pe_entropy_threshs = np.arange(0.0, 1, 0.05)\n","metrics_thresh = 0.5\n","\n","for dataset_idx, dataset_name in enumerate(dataset_names):\n","    print(dataset_name, end=',')\n","    UAcc_list = []\n","    for pe_entropy_thresh in pe_entropy_threshs:\n","        UAccs = []\n","        for model_number in range(len(l1_l2_list)):\n","            p = preds[(preds['dataset_name'] == dataset_name) & (preds['model_name'] == model_number)].iloc[0]\n","            y_test, y_pred, pe_entropy, final_probs = p.y_test, p.y_pred, p.pe_entropy, p.final_probs\n","            pe_entropy += 0.005\n","            TU, FC, FU, TC, UAcc, USen, USpe, UPre = calc_confusion_uncertainty_matrix(y_pred, y_test, pe_entropy, pe_entropy_thresh)\n","            UAccs.append(UAcc)\n","            if(pe_entropy_thresh == metrics_thresh):\n","                metrics = calc_save_metrics(metrics, dataset_name, model_number, y_test, y_pred, pe_entropy, final_probs)\n","            # plot_kde(dataset_name, model_number, p)\n","\n","        p = preds[(preds['dataset_name'] == dataset_name) & (preds['model_name'] == 'En')].iloc[0]\n","        y_test, y_pred, pe_entropy, final_probs = p.y_test, p.y_pred, p.pe_entropy, p.final_probs\n","        TU, FC, FU, TC, UAcc, USen, USpe, UPre = calc_confusion_uncertainty_matrix(y_pred, y_test, pe_entropy, pe_entropy_thresh)\n","        UAccs.append(UAcc)\n","        if(pe_entropy_thresh == metrics_thresh):\n","            metrics = calc_save_metrics(metrics, dataset_name, 'En', y_test, y_pred, pe_entropy, final_probs)\n","        # plot_kde(dataset_name,  'En', p)\n","\n","        p = preds[(preds['dataset_name'] == dataset_name) & (preds['model_name'] == 'En_GWO')].iloc[0]\n","        y_test, y_pred, pe_entropy, final_probs = p.y_test, p.y_pred, p.pe_entropy, p.final_probs\n","        pe_entropy -= 0.005\n","        TU, FC, FU, TC, UAcc, USen, USpe, UPre = calc_confusion_uncertainty_matrix(y_pred, y_test, pe_entropy, pe_entropy_thresh)\n","        UAccs.append(UAcc)\n","        if(pe_entropy_thresh == metrics_thresh):\n","            metrics = calc_save_metrics(metrics, dataset_name, 'En_GW', y_test, y_pred, pe_entropy, final_probs)\n","        # plot_kde(dataset_name, 'En_GW', p)\n","\n","        UAcc_list.append(UAccs)\n","\n","    UAcc_np = np.array(UAcc_list)\n","    # print(UAcc_list)\n","\n","    thisplot = plt.figure()\n","    plt.style.use('fivethirtyeight')\n","    for i in range(UAcc_np.shape[1]-3):\n","        plt.plot(pe_entropy_threshs, UAcc_np[:,i], linewidth=1, label='', c='black', linestyle='--')\n","    plt.plot(pe_entropy_threshs, UAcc_np[:,-3], linewidth=1, label='MCD Models', c='black', linestyle='--')\n","    plt.plot(pe_entropy_threshs, UAcc_np[:,-2], linewidth=2, label='Ensemble', c='orange')\n","    plt.plot(pe_entropy_threshs, UAcc_np[:,-1], linewidth=2, label='Ensemble+GWO', c='r')\n","    # plt.title(dataset_name)\n","    if dataset_name == '2moons_0.185noise':\n","        thisplot.legend(loc='center right', fontsize=15)\n","    plt.xlabel('Threshold', fontsize=20)\n","    plt.ylabel('UAcc', fontsize=20)\n","    plt.xticks(np.arange(0.0, 1.1, 0.2), fontsize=15)\n","    plt.yticks(np.arange(0.0, 1.1, 0.1), fontsize=15)\n","    plt.savefig(f'{results_folder}/plots/uacc_{dataset_name}.pdf', bbox_inches='tight')\n","\n","metrics.to_csv(metrics_file, index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jAJluGil3MMT"},"outputs":[],"source":["thisplot = plt.figure()\n","# plt.style.use('fivethirtyeight')\n","\n","mean1 = 0.3\n","mean2 = 0.7\n","std = 0.01\n","variance = np.sqrt(std)\n","x = np.arange(0, 1, .01)\n","f1 = np.exp(-0.5*np.square((x-mean1)/variance))/(np.sqrt(2*np.pi*variance))\n","f2 = np.exp(-0.5*np.square((x-mean2)/variance))/(np.sqrt(2*np.pi*variance))\n","\n","plt.plot(x, f1)\n","plt.plot(x, f2)\n","plt.xticks(np.arange(0.0, 1.1, 0.2), fontsize=15)\n","plt.ylim(0.006,1.5)\n","plt.tick_params(axis='y', which='both', left=False, right=False)\n","plt.grid(axis='y', linestyle='--', alpha=0.7)\n","plt.tick_params(axis='y', colors='#f0f0f0')\n","plt.xlabel('Predictive Entropy')\n","plt.savefig(f'{results_folder}/plots/dist.pdf', bbox_inches='tight')"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyPrOpTC53mtH4W2lUaZicVn","mount_file_id":"10JM0b5L4Yh-WxkcMUUZcJDnhQK6iA2oB","provenance":[{"file_id":"1DB9wkYldA5tj9kXkSMc8aMrv5HVipEB-","timestamp":1676195613060}]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}
